Build a complete, standalone desktop application called ‘CousinPrankApp’ that performs real-time face swapping and voice cloning for prank video calls. The app must be 100% functional out of the box, with no errors, and run entirely locally on a standard Windows/Mac/Linux machine with a GPU (NVIDIA preferred, 6GB+ VRAM). Use Python as the base language, bundle all dependencies, and package it as a single executable (.exe for Windows, .app for Mac, or cross-platform via PyInstaller/Electron) that users can double-click to launch.
Key requirements:
•  Input: User drops exactly one photo (JPG/PNG) of the target (e.g., cousin’s face) and one short audio clip (10-30 seconds, WAV/MP3) of their voice.
•  Real-Time Processing: Capture the user’s live webcam video and microphone audio. Swap the user’s face with the target’s face from the single photo (transferring expressions, head movements, blinking, mouth movements for lip-sync). Clone the user’s spoken words into the target’s voice timbre and prosody.
•  Output: Feed the processed video and audio to virtual camera and microphone devices (e.g., using pyvirtualcam and VB-Audio or BlackHole) so it can be selected in any video call app like Zoom, Discord, or FaceTime.
•  Latency: Under 300ms end-to-end for smooth live use; optimize for consumer hardware.
•  User Interface: Simple GUI (use Tkinter or PyQt) with:
	•  Buttons to upload the photo and audio.
	•  ‘Start Prank’ button to begin real-time cloning.
	•  Preview window showing the swapped output.
	•  ‘Stop’ button and basic settings (e.g., toggle lip-sync enhancement).
•  Core Technologies (must use these for zero-training, one-shot cloning):
	•  Face Swapping: Deep-Live-Cam (GitHub: hacksider/Deep-Live-Cam) for real-time, one-photo face animation and expression transfer.
	•  Voice Cloning: OpenVoice (GitHub: myshell-ai/OpenVoice) for instant, real-time speech-to-speech cloning from one audio sample.
	•  Video/Audio Handling: OpenCV for webcam capture, sounddevice/PyAudio for mic, FFmpeg for any sync needs.
	•  Integration: Threading for parallel video/audio processing to avoid lag.
•  No Training Required: Everything must work with just one photo (zero-shot face) and one audio (zero/few-shot voice); no dataset collection or long training times.
•  Edge Cases: Handle low-light, head turns up to 60 degrees, multiple faces (focus on primary), and audio noise. Include error handling (e.g., ‘No GPU detected’ message).
•  Ethical Safeguards: Add a startup disclaimer popup: ‘For pranks only; get consent and don’t use for harm. Outputs are fakes.’
•  Packaging: Include all models and libs in the build (auto-download on first run if needed). Make it open-source friendly but provide the full source code alongside the executable.
•  Testing: Ensure it works on a sample photo/audio (include defaults in the app). Output must fool casual viewers in short calls.
•  Deliverables: Provide the full source code, the packaged executable, step-by-step install/run instructions, and a demo video/gif showing it in action on Zoom.
Make this 100% functional and bug-free—test thoroughly before delivering. If any part can’t be implemented exactly as described, explain why and suggest fixes.”